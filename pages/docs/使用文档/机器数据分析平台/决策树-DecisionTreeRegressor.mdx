用决策树做回归任务时，从根节点开始，对样本的某一特征进行测试，根据测试结果，将样本分配到其子结点；这时，每一个子节点对应着该特征的一个取值。如此递归地对样本进行测试并分配，直至到达叶结点。在回归树中，采用的是启发式的方法。假如我们有n个特征，每个特征有s个取值，那我们遍历所有特征，尝试该特征所有取值，对空间进行划分，直到取到特征j的取值s，使得损失函数最小。

**语法：**
```
..| fit DecisionTreeRegressor splitter=<best(default) | random> criterion=<mse(default) | mae | friedman_mse> max_features=<auto(default) | sqrt | log2 | None | int | float> max_depth=<int> min_samples_split=<int | float> min_samples_leaf=<int | float> max_leaf_nodes=<int> random_state=<int> <target_field> from <feature_field_1> <feature_field_2>... 
```

**参数说明：**

- splitter用来决定划分点选择标准，
	+ splitter=best, 在特征的所有划分点中找出最优的划分点，适用于样本量不大的情况。
	+ splitter=random，随机的在部分划分点中找局部最优的划分点，适用于样本数量非常大的情况。

- criterion用来决定CART树做划分时对特征的评价标准。
	+ criterion=mse，使用均方误差。
	+ criterion=mae, 使用平均绝对误差。
	+ criterion=friedman_mse（默认值），使用Friedman改进分数的均方误差函数。

- max_features用来决定划分时考虑的最大特征数。
	+ max_features=int，代表直接使用max_features为最大特征数量，必须在(0,n_features]之间。n_features即为建模时使用的特征字段的数量。
	+ max_features=float，代表使用max_features\*n_features为最大特征数量。必须使max_features\*n_features向下取整的结果在(0,n_features]之间。
	+ max_features=sqrt, 代表使用sqrt(n_features)，即n_features的平方根值为最大特征数量。
	+ max_features=log2, 代表使用log2(n_features)。
	+ max_features=auto（默认值），和max_features=sqrt一样。

- max_depth用来决定数的最大深度。若不提供，即为不设限。在数据量大以及使用的特征数量也很大的情况，可以考虑将max_depth设为[0,100]之间。

- min_samples_split用来决定分割一个内部节点所需要的最小样本数量。如果某节点的样本数少于min_samples_split，则不会再继续划分。在数据量非常大的情况，建议增大这个值。
	+ min_samples_split=int，代表直接使用min_samples_split作为最小样本数量。
	+ min_samples_split=float，代表使用min_samples_split\*n_samples向上取整的结果作为最小样本数量。n_samples即为建模时使用的样本数据量。
	+ min_samples_split默认为2。
	
- min_samples_leaf用来决定需要在叶子节点上的最小样本数量。如果某叶子节点数目小于样本数，则会和兄弟节点一起被剪枝。在数据量非常大的情况，建议增大这个值。
	+ min_samples_leaf=int，代表直接使用min_samples_leaf作为最小样本数量。
	+ min_samples_leaf=float，代表使用min_samples_leaf\*n_samples向上取整的结果作为最小样本数量。n_samples即为建模时使用的样本数据量。
	+ min_samples_leaf默认为1。

- max_leaf_nodes用来决定最大叶子结点数量。若不提供，则不设限。通过限制最大叶子节点数，可以防止过拟合。在特征数量非常大的情况，建议增大这个值。

- random_state为随机数种子，用来控制样本自助抽样的随机性和特征抽样的随机性。如果给定特定值，重新跑模型的时候，可以得出同样的结果。

