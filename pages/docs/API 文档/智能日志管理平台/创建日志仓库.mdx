**请求语法**

```
POST /v5/repos/<RepoName>
Content-Type: application/json
Authorization: Pandora <auth>
{
  "region": <Region>,
  "retention": <Retention>,
  "fullText":{
    "enabled":true, 
    "analyzer":""  
  },
  "schema": [
    {
      "key": <Key>,
      "valtype": <ValueType>,
      "analyzer":<AnalyzerName>
    },
    "similarity":{
    	"name":<Name>,
    	"options":{
    		"type":<Type>,
    		<Key>:<Value>,
    		....
    	}
    }
    ...
  ]
}
```

**请求内容**

|参数|类型|必填|说明|
|:---|:---|:---:|:---|
|RepoName|string|是|日志仓库名称，用来标识该日志仓库的唯一性；</br>命名规则: `^[a-z][a-z0-9_]{0，127}$`|
|region|string|是|所属区域,计算与存储所使用的物理资源所在区域,目前支持华东区域(代号`nb`)；</br>此参数是为了降低用户传输数据的成本，应当尽量选择离自己数据源较近的区域|
|retention|string|是|数据存储时限，1天=`1d`，最大支持30天|
|fullText.enabled|bool|否|是否开启全文索引，默认为false，不开启全文索引|
|fullText.analyzer|string|否|全文索引使用的分词器，和schema.analyzer意义相同，见`analyzers`定义。|
|schema|json|是|字段信息|
|key|string|是|字段名称，用来标识该字段的唯一性；</br>命名规则: `^[a-zA-Z_][a-zA-Z0-9_]{0,60}$`|
|valtype|string|是|字段类型，目前支持`string`、`float`、`long`、`boolean`,`date`，`ip`,`geo_point`和`object`共8种类型；</br>其中`date`支持`RFC3339Nano`和`RFC3339Nano(Numeric time zone offsets format)`，</br>例：`2006-01-02T15:04:05.999999999Z07:00`和`2006-01-02T15:04:05.999999999+08:00`;`geo_point`为经纬度坐标，如 `[ -71.34, 41.12 ]`|
| schema.analyzer |string|否|文本分词方式，支持`standard`,`whitespace`,`index_ansj`(中文分词),`keyword`,`no`5种内置分词方式；同时支持`pattern`类型的自定义分词器，见`analyzers`定义。| 

**schema.valtype.object 类型：**

`object`是一个json对象，下面例子中的`work`字段即为`object`类型，其中`key`为`string`类型，`value`的类型会根据实际数据进行推断。凡是`object`类型的字段会被自动索引，被索引后，可以通过`work.address`作为key进行搜索。

```
   [
        {
            "name":"testName",
            "work":{
                "address":"workAddress",
                "name":"workName",
                "salary":111.00
            },
            "timestamp":"2016-05-10T15:00:00+08:00"
        }
    ]   
```

**示例**

```
curl -X POST https://nb-insight.qiniuapi.com/v5/repos/test_repo \
-H 'Content-Type: application/json' \
-H 'Authorization: Pandora 2J1e7iG13J66GA8vWBzZdF-UR_d1MF-kacOdUUS4:NTi3wH_WlGxYOnXsvgUrO4XMD6Y=' \
-d '{
  "region": "nb",
  "retention": "1d"
  "schema": [
    {
      "key": "userName",
      "valtype": "string",
    },
    {
      "key": "createDate",
      "valtype": "date"
    }
  ]
}'
```

**分词方式详解:**

分词器简单来说定义了两个方面：

	1. 如何将一段文档划分为多个词
	2. 如何对这些词做过滤和处理

	
下面通过阐述一个简单的例子来对分词器如何工作做一个简单的说明。

实例文档：
	
	The 2 QUICK Brown-Foxes jumped over the lazy dog's bone.


简单来说文档在索引的过程中会有以下两个个步骤，了解这个步骤有助于用户理解如何更好的使用LOGDB进行搜索。

	1. 解析此document为多个token，token通常指以非字母数字结束的一个字符串（还有其他的区分方式，比如）,比如'apple+orange'则为两个token 'apple' 'orange'
	2. 进入TokenFilter阶段，即是对每个Token根据所选择的分词器做过滤处理。比如下面几个常见的filter
			
			* lowercase filter，将所有字母变为小写
			* stopword filter， 过滤掉所有定义的stopword，一些常见的比如 'a','the'等
			* 其他特殊的filter。


#### **`standard`**
	
	划分document时以unicode字符作为结束的标志，然后过滤掉大部分标点符号，将所有字母变为小写。如果不清楚选择哪个分词器，默认使用这个。
	
	unicode字符：详见http://unicode.org/reports/tr29/
		
	standard分词器处理后，实例文档变为:`the, 2, quick, brown, foxes, jumped, over, the, lazy, dog's, bone`几个词

#### **`keyword`**
	document不划分，整体作为个token。如果字段为ID、国家等类似的时，强烈建议选择此分词器

#### **`whitespace` **
	划分document时，碰到空格即认为一个token的结束
	
	whitespace分词器处理后，实例文档变为:  `The, 2, QUICK, Brown-Foxes, jumped, over, the, lazy, dog's, bone.`
	
	
	keyword分词器处理后，实例文档变为: `The 2 QUICK Brown-Foxes jumped over the lazy dog's bone.`。 注意：此时搜索单个单词无法匹配到该数据，只能输入全部的docuemtn才能成功匹配。

#### **`no`**
	不分词不索引。 不能直接通过条件搜索到
	
#### **`index_ansj`**
	 根据中文语义分词，目前基于我们的系统词库来做分词依据

